{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columbia University\n",
    "### ECBM E4040 Neural Networks and Deep Learning. Fall 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1, Task 4: Questions (10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 \n",
    "\n",
    "What is the effect of increasing the number of layers in an MLP? Based on your experiments in task 2/3, are larger models (with a greater number of hidden layers) preferred to smaller ones? Why or why not?\n",
    "\n",
    "   Your answer: \n",
    "#### Answer:\n",
    "  #### Q1:\n",
    "   1. Representational Power: Deeper networks generally have greater representational power. They can capture more complex relationships and hierarchies in the data. For instance, in the context of image recognition, initial layers might capture edges and textures, while deeper layers capture more complex structures and patterns.\n",
    "   2. Risk of Overfitting: Adding more layers also increases the number of parameters in the model. This can make the network more prone to overfitting, especially if you have limited training data. Overfitting occurs when the model starts to memorize the training data instead of generalizing from it.\n",
    "   3. Computational Complexity: The training time and computational resources required increase with the addition of more layers. This can be a concern especially for very deep networks or when training on large datasets.\n",
    "   4. Vanishing & Exploding Gradients: Deeper networks can be harder to train due to the vanishing and exploding gradient problems. These issues can make gradients (used in backpropagation to update the weights) either too small (vanish) or too large (explode), causing training instability. This has been somewhat mitigated by better weight initialization techniques and activation functions, but it remains a consideration.\n",
    "   5. Need for Regularization: Deeper models might require the use of regularization techniques like dropout, batch normalization, or L2 regularization to prevent overfitting and stabilize training.\n",
    "   #### Q2: \n",
    "   Prefered smaller models. Smaller models might be preferred to prevent overfitting and achieve faster training times. Larger models will achieve a overfitting in the small dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 \n",
    "\n",
    "What is the significance of activation functions in deep learning models? Name two activation functions that you can use for a hidden layer **and** two for the output layer. \n",
    "\n",
    "   Your answer:\n",
    "#### Answer:\n",
    "Activation functions play a crucial role in deep learning models for several reasons:\n",
    "\n",
    "1. **Non-linearity**: One of the primary purposes of an activation function is to introduce non-linearity into the network. This allows the model to learn from the error and make adjustments, which is essential for learning complex patterns. Without non-linear activation functions, the entire network would behave as a linear model, limiting its capacity to only model linear relationships.\n",
    "\n",
    "2. **Gradient Propagation**: Activation functions and their derivatives (used in backpropagation) facilitate the updating of weights throughout the network. A good activation function will allow gradients to flow smoothly, helping the network to learn effectively.\n",
    "\n",
    "3. **Bounding Outputs**: Some activation functions squash their outputs into a range, like [0, 1] for the sigmoid function or [-1, 1] for the tanh function. This can be useful in certain scenarios to ensure that outputs don't reach extremely high or low values.\n",
    "\n",
    "4. **Sparsity**: Activation functions like the ReLU (Rectified Linear Unit) introduce sparsity. ReLU and its variants set all negative values to zero, which means that it activates neurons (i.e., produces a non-zero output) only when positive inputs are received. This sparsity is beneficial for the efficient training and robustness of the model.\n",
    "\n",
    "For **hidden layers**, two common activation functions are:\n",
    "\n",
    "1. **ReLU (Rectified Linear Unit)**: It's defined as $$ f(x) = max(0, x) $$. It's the most widely used activation function due to its simplicity and efficiency. Variants like Leaky ReLU and Parametric ReLU have also been proposed to address some of its limitations.\n",
    "\n",
    "2. **tanh (Hyperbolic Tangent)**: It's defined as $$ f(x) = \\frac{2}{1 + e^{-2x}} - 1 $$. The tanh function outputs values in the range of [-1, 1], making it zero-centered and sometimes preferred over the sigmoid function in hidden layers.\n",
    "\n",
    "For the **output layer**, the choice of activation function depends on the specific task:\n",
    "\n",
    "1. **Sigmoid**: For binary classification problems, the sigmoid function is often used as it maps its input to a value between 0 and 1, which can be interpreted as a probability.\n",
    "\n",
    "2. **Softmax**: For multi-class classification problems, the softmax function is used. It converts the raw output scores (logits) from the network into probability distributions over the classes.\n",
    "\n",
    "Remember, the choice of activation function not only depends on its position (hidden layer vs. output layer) but also the specific problem you're trying to solve. For tasks like regression, you might use a linear (or no) activation function in the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Assume you have a problem to predict the annual rainfall in a certain region with some historic numeric data that was given to you, which of these 2 models (Linear Regression vs. Logistic Regression) would you use? How would you modify the problem statement to use the other model? Do they both adopt a linear decison boundary? \n",
    "\n",
    "   Your answer: \n",
    "#### Answer:\n",
    "a Linear Regression model is more suitable. This is because rainfall prediction is a regression problem where the output is a continuous value (amount of rainfall).\n",
    "\n",
    "**Logistic Regression**, on the other hand, is used for binary classification problems. It predicts the probability that a given instance belongs to a particular category. The output of logistic regression is a value between 0 and 1, which can be interpreted as the probability of the instance belonging to the positive class.\n",
    "\n",
    "To use **Logistic Regression** for this problem, you'd need to modify the problem statement to make it a classification task. For instance:\n",
    "\n",
    "* Predict whether the annual rainfall in a certain region will be above average (1) or below average (0) based on historic numeric data.\n",
    "Regarding the decision boundary:\n",
    "\n",
    "* **Linear Regression** does not have a concept of a \"decision boundary\" in the same way classification models do. It tries to find the best linear relationship (line in case of 2D, plane in case of 3D, and hyperplane in higher dimensions) that fits the data.\n",
    "* **Logistic Regression** adopts a linear decision boundary, even though the relationship between independent variables and the predicted probability is non-linear (due to the logistic/sigmoid function). What this means is that in a 2D space, the decision boundary would be a straight line. In a 3D space, it would be a plane, and so forth.\n",
    "\n",
    "In essence, while Logistic Regression outputs probabilities through a non-linear transformation (the logistic function), the decision boundary itself, which separates one class from another, is linear in the feature space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "What will happen if you choose a very small or a very large learning rate? \n",
    "\n",
    "   Your answer: \n",
    "#### Answer:\n",
    "The learning rate in optimization algorithms, especially gradient-based methods like gradient descent, is a crucial hyperparameter. Its value affects the convergence and performance of the algorithm. Here's what generally happens with very small or very large learning rates:\n",
    "\n",
    "1. **Very Small Learning Rate**:\n",
    "\n",
    "   - **Slow Convergence**: A small learning rate means the model makes tiny adjustments to the weights during each iteration. As a result, it might take a significantly larger number of iterations to converge to the minimum.\n",
    "   \n",
    "   - **Risk of Getting Stuck in Local Minima**: If the learning rate is too small, the model might get stuck in local minima or saddle points because it doesn't take large enough steps to escape those areas, especially in complex loss landscapes associated with deep neural networks.\n",
    "   \n",
    "   - **More Stable Convergence**: While the convergence is slow, it tends to be more stable and less oscillatory. The updates are more refined and precise, reducing the chances of overshooting.\n",
    "   \n",
    "2. **Very Large Learning Rate**:\n",
    "\n",
    "   - **Overshooting**: A large learning rate means the model makes big adjustments to the weights during each iteration. This can lead to overshooting the optimal point in the loss landscape.\n",
    "   \n",
    "   - **Divergence**: In extreme cases, instead of converging, the loss might diverge to infinity, meaning the model fails to learn anything meaningful.\n",
    "   \n",
    "   - **Oscillation**: Even if it doesn't diverge, the model might oscillate around the minimum, never truly converging.\n",
    "   \n",
    "   - **Risk of Skipping Optimal Solutions**: With too large a step, the optimization process might skip over valleys or narrow regions in the loss landscape that represent better solutions.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "What is the interpretation of **perplexity** in t-SNE? How did you set this value during the tuning in task3?\n",
    "    \n",
    "   Your answer:\n",
    "#### Q1:\n",
    "1. **Balance Between Preserving Local and Global Structure**: Perplexity can be roughly understood as a measure that determines how to balance attention between preserving the local and global structures in the data. A low perplexity emphasizes preserving local data structures, whereas a high perplexity gives more importance to the global structures.\n",
    "\n",
    "2. **Effective Number of Neighbors**: Perplexity can also be viewed as a knob that sets the effective number of neighbors t-SNE considers for each point. For example, a perplexity of 30 would mean that each point roughly considers 30 other points as its neighbors.\n",
    "\n",
    "3. **Stability and Clusters**: The choice of perplexity can influence the stability and appearance of clusters in the t-SNE visualization. Too low a value might result in isolated clusters that don't truly represent the underlying data distribution, while too high a value can merge clusters that should be distinct.\n",
    "\n",
    "#### Q2:\n",
    "1. **Experimentation**: There isn't a one-size-fits-all perplexity value. It's often recommended to try multiple perplexity values to see which one gives a representation that makes the most sense for your data.\n",
    "\n",
    "2. **Common Range**: Perplexity values between 5 and 50 are common, but this can vary based on the size and nature of the dataset.\n",
    "\n",
    "3. **Visual Inspection**: After generating t-SNE visualizations with different perplexity values, visually inspect the results. The best value often depends on which visualization best captures meaningful patterns or clusters in the data.\n",
    "\n",
    "4. **Stability Test**: A good practice is to run t-SNE multiple times (due to its random initialization) for each perplexity value to ensure the results are stable and not a random artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
